{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pandas: La librairie d'analyse de données\n",
    "[Pandas](https://pandas.pydata.org) est la bibliothèque de référence en langage de programmation Python quand il s'agit de devoir analyser et manipuler des données grâce à plusieurs points forts :\n",
    "\n",
    "- des outils pour lire et écrire des données, qu'elles soient en mémoires (dictionnaires Python ou arrays Numpy par exemple) ou qu'elles proviennent de différents formats: fichiers CSV (comma-separated values) et texte, Microsoft Excel, bases de données SQL ou encore le format rapide HDF5 (Hierarchical Data Format 5)\n",
    "- un alignement \"intelligent\" des données et un traitement intégré des données manquantes \n",
    "- un remodelage et pivotement flexibles des ensembles de données afin de pouvoir les réordonner selon l'envie\n",
    "- un découpage basé sur les étiquettes des données, ce qui facilite la compréhension du code\n",
    "- la possibilité d'insérer et de supprimer des colonnes ou lignes des structures de données\n",
    "- la possibilité d'aggréger et de transformer des données avec un puissant moteur de regroupement permettant d'effectuer des opérations de fractionnement, de combinaison et d'application de fonction\n",
    "- des performances de calculs hautement optimisées, à l'aide d'un noyau codé en C au même titre que la librairie Numpy\n",
    "- elle s'interface parfaitement avec d'autres librairies Python utiles en Data Science\n",
    "- sa syntaxe de haut niveau, se rapprochant de l'anglais comme Python, la rend facile à utilisé et accessible à tous\n",
    "- elle est open source, distribuée sous une licence BSD 3-Clause et maintenue publiquement sur [GitHub](https://github.com/pandas-dev/pandas) par plus de 2000 contributeurs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## La classe DataFrame\n",
    "La classe d'objets que nous allons principalement utiliser est la classe **DataFrame** de Pandas, qui se présente sous la forme d'une matrice dont chaque ligne et chaque colonne porte un indice. Elle s'apparente à un tableau qui propose une manipulation et exploration de données plus avancées que les arrays Numpy, notamment par le fait que chacune de ses lignes et chacune de ses colonnes portent un indice qui peut être personnalisé. Son utilisation principale est de stocker des bases de données, dont les différentes **entrées** (individus, dossiers, clics, ...) sont les différentes **lignes** et les différentes **caractéristiques** sont les différentes **colonnes**.\n",
    "\n",
    "Concernant l'analyse de données, cette classe présente plusieurs avantages par rapport à un la classe array de Numpy :\n",
    "- un DataFrame est beaucoup plus lisible grâce à une indexation des colonnes et des lignes plus explicites\n",
    "- le type des éléments peut varier d'une colonne à l'autre (même si au sein d'une même colonne, les éléments sont du même type), ce qui permet de stocker des bases de données très variées contrairement aux arrays Numpy, qui ne supportent que des données du même type au sein de toute la matrice\n",
    "- elle contient beaucoup plus de méthodes pour le nettoyage, le pré-traitement et la manipulation de bases de données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "from pandas.core.common import SettingWithCopyWarning\n",
    "warnings.filterwarnings(\"ignore\", category = SettingWithCopyWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import de la bibliothèque Pandas sous l'alias pd (c'est une convention dans la communauté des développeurs Python)\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Création d'un DataFrame\n",
    "Un DataFrame Pandas peut être instantié de plusieurs manières, la principale étant à l'aide du constructeur suivant et ses 3 arguments principaux ([il y en a plus](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.html)) :\n",
    "```python\n",
    "pd.DataFrame(data, index, columns, ...)\n",
    "\n",
    "```\n",
    "- le paramètre `data` contient les **données** à mettre en forme (array NumPy, liste, dictionnaire ou un autre DataFrame).\n",
    "- le paramètre `index`, si précisé, doit être une **liste** contenant les **indices des entrées**.\n",
    "- le paramètre `columns`, si précisé, doit être une **liste** contenant le **nom des colonnes**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Création d'un DataFrame à partir d'un array Numpy\n",
    "L'inconvénient de cette méthode est que le type de données est obligatoirement numérique et doit être le même pour l'ensemble du jeu de données, ce qui fait perdre à la classe DataFrame sa flexibilité par rapport aux arrays Numpy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import de la librairie Numpy sous l'alias np\n",
    "import numpy as np\n",
    "\n",
    "# Création d'un array Numpy\n",
    "array = np.array([[1, 2, 3, 4], \n",
    "                  [5, 6, 7, 8], \n",
    "                  [9, 10, 11, 12]])\n",
    "\n",
    "# Instanciation d'un DataFrame nommé df à partir de l'array Numpy\n",
    "df = pd.DataFrame(data = array)\n",
    "\n",
    "# Affichage du DataFrame df\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Affichage propre en sortie de la cellule en utilisant l'interface IPython de Jupyter Notebook\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour que cet affichage propre se fasse, il faut que le nom du DataFrame, ici `df`, soit la dernière instruction de la celulle de code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instanciation d'un DataFrame nommé df à partir de l'array Numpy en précisant les indices et le nom des colonnes dans une liste\n",
    "df = pd.DataFrame(data = array,\n",
    "                  index = ['premier indice', 'deuxième indice', 'troisième indice'],\n",
    "                  columns = ['première caractéristique', 'deuxième caractéristique', 'troisième caractéristique', 'quatrième caractéristique'])\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Création d'un DataFrame à partir d'un dictionnaire\n",
    "L'avantage de cette méthode par rapport à la précédente est de pouvoir stocker des données de plusieurs types différents dans chaque colonne. Les colonnes sont d'ailleurs directement définies dans le dictionnaire et correspondent aux clés des éléments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Création d'un dictionnaire\n",
    "dict_CentraleIA = {'Nom': ['Benichou', 'El Mahdi', 'Lenglet', 'Peyré', 'Zaamoun', 'Skewes', 'Caron', 'Purim'], \n",
    "                'Prénom': ['Yaniv', 'Karim', 'Marie', 'Antoine', 'Amine', 'Pablo', 'Mathieu', 'Andreis'],\n",
    "                'Sexe': ['M', 'M', 'F', 'M', 'M', 'M', 'M', 'M'],\n",
    "                'Taille': [186, 172, 170, 175, 176, 177, 173, 180],\n",
    "                'Âge': [22, 20, 21, 20, 21, 22, 20, 22],\n",
    "                'Rôle': ['Prez', 'Vice-Prez', 'Screz', 'Trez', 'Respo Formation', 'Respo Evénement', 'Respo Com', 'Respo Entreprise']}\n",
    "\n",
    "# Instanciation d'un DataFrame \n",
    "df_CentraleIA = pd.DataFrame(data = dict_CentraleIA,\n",
    "                  index = ['i_1', 'i_2', 'i_3', 'i_4', 'i_5', 'i_6', 'i_7', 'i_8'])\n",
    "\n",
    "df_CentraleIA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Création d'un DataFrame à partir d'un fichier de données\n",
    "En général, les DataFrame sont directement créés à partir de fichier contenant les données voulues, que ce soit du format CSV, Texte ou encore Excel, même si le format CSV (Comma Separated Values) est le plus courant. Par rapport à l'exemple précédent sur les membres du bureau actuel de l'association Centrale IA, le fichier aurait ressemblé à ça :\n",
    "```csv\n",
    "Nom, Prénom, Sexe, Taille, Âge, Rôle\n",
    "Benichou, Yaniv, M, 186, 22, Prez\n",
    "El Mahdi, Karim, M, 175, 22, Vice-Prez\n",
    "Lenglet, Marie, F, 175, 22, Screz\n",
    "Peyré, Antoine, M, 175, 22, Trez\n",
    "Zaamoun, Amine, M, 176, 22, Respo Formation\n",
    "Skewes, Pablo, M, 176, 22, Respo Evénement\n",
    "Caron, Mathieu, M, 173, 20, Respo Com\n",
    "Purim, Andreis, M, 180, 22, Respo Entreprise\n",
    "```\n",
    "On peut remarquer que :\n",
    "- chaque ligne correspond à une entrée de la base de données, donc dans ce cas, les entrées correspondent à des membres du bureau de Centrale IA\n",
    "- la première ligne contient le nom des colonnes, même s'il est tout à fait possible d'avoir un fichier CSV sans que le nom des colonnes ne soit spécifié\n",
    "- les valeurs sont séparées par un caractère de séparation. Dans cet exemple, il s'agit de la virgule ',' mais il peut très bien s'agir du point-virgule ';'\n",
    "\n",
    "Pour importer des données provenant d'un fichier de ce format (ou du format `.data` que l'on peut retrouver dans plusieurs jeux de données [ici](http://archive.ics.uci.edu/ml)), il faut utiliser la [fonction suivante](https://pandas.pydata.org/docs/reference/api/pandas.read_csv.html) :\n",
    "```python\n",
    "pd.read_csv(filepath_or_buffer, sep, header, index_col, ...)\n",
    "```\n",
    "Ses arguments incontournables sont :\n",
    "- `filepath_or_buffer`: Le chemin d'accès du fichier .csv relativement à l'environnement d'éxécution. Si le fichier se trouve dans le même dossier que l'environnement Python, il suffit de renseigner le nom du fichier. Ce chemin doit être renseigné sous forme de chaîne de caractères.\n",
    "- `sep`: Le caractère utilisé dans le fichier .csv pour séparer les différentes colonnes. Cet argument doit être specifié sous forme de caractère.\n",
    "- `header`: Le numéro de la ligne qui contient les noms des colonnes. Si par exemple les noms de colonnes sont renseignés dans la première ligne du fichier .csv, alors il faut spécifier header = 0. Si les noms ne sont pas renseignés, on mettra header = None.\n",
    "- `index_col`: Le nom ou numéro de la colonne contenant les indices de la base de données. Si les entrées de la base sont indexées par la première colonne, il faudra renseigner index_col = 0. Alternativement, si les entrées sont indexées par une colonne qui porte le nom \"Id\", on pourra spécifier index_col = \"Id\".\n",
    "\n",
    "Cette fonction retournera un objet de type DataFrame contenant toutes les données du fichier importé."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import du fichier 'german_credit_risk.data' dans un DataFrame nommé df\n",
    "df = pd.read_csv('german_credit_risk.data', sep = ' ', header = None, index_col = None)\n",
    "\n",
    "# Affichage du DataFrame df en sortie\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le [jeu de données](https://www.kaggle.com/kabure/german-credit-data-with-risk) que nous venons d'importer représente classe des individus décrits par un ensemble d'attributs en tant que bons ou mauvais risques de crédit. Il peut typiquement à faire ce qu'on appelle du \"scoring\" d'octroi de crédit, c'est-à-dire concevoir un modèle statistique permettant de prédire si une personne est solvable ou non, en fonction de ses caractéristiques. La partie modélisation sera abordée dans un autre formation plus tard concernant les modèles de Machine Learning.\n",
    "\n",
    "Pour le moment, continuons à en apprendre plus sur la librairie Pandas tout en nous intéressons particulièrement aux données qui constituent ce fichier et essayons de se l'approprier en l'analysant statistiquement. Cette étape d'appropriation du jeu de données avec lequel on travaille constitue près de 80% du travail du Data Scientist et permet de modéliser au mieux car une bonne connaissance des données avec lesquelles nous travaillons est primordial.\n",
    "\n",
    "Les caractéristiques des individus et leurs types sont les suivants :\n",
    "\n",
    "Attribute 1: (qualitative)\\\n",
    "**Status of existing checking account** (*Statut du compte chèque existant*)\n",
    "- A11 : ... < 0 DM\n",
    "- A12 : 0 <= ... < 200 DM\n",
    "- A13 : ... >= 200 DM / salary assignments for at least 1 year\n",
    "- A14 : no checking account\n",
    "    \n",
    "Attribute 2: (numerical)\\\n",
    "**Duration** in month (*Durée en mois*)\n",
    "\n",
    "Attribute 3: (qualitative)\\\n",
    "**Credit history** (*Historique des crédits*)\n",
    "- A30 : no credits taken/ all credits paid back duly\n",
    "- A31 : all credits at this bank paid back duly\n",
    "- A32 : existing credits paid back duly till now\n",
    "- A33 : delay in paying off in the past\n",
    "- A34 : critical account / other credits existing (not at this bank)\n",
    "    \n",
    "Attribute 4: (qualitative)\\\n",
    "**Purpose** (*Raison pour laquelle le crédit a été accordé*)\n",
    "- A40 : car (new)\n",
    "- A41 : car (used)\n",
    "- A42 : furniture/equipment\n",
    "- A43 : radio/television\n",
    "- A44 : domestic appliances\n",
    "- A45 : repairs\n",
    "- A46 : education\n",
    "- A47 : (vacation - does not exist?)\n",
    "- A48 : retraining\n",
    "- A49 : business\n",
    "- A410 : others\n",
    "\n",
    "Attribute 5: (numerical)\\\n",
    "**Credit amount** (*Montant du crédit*)\n",
    "\n",
    "Attibute 6: (qualitative)\\\n",
    "**Savings account/bonds** (*Compte d'épargne/obligations*)\n",
    "- A61 : ... < 100 DM (Deutsche Mark)\n",
    "- A62 : 100 <= ... < 500 DM\n",
    "- A63 : 500 <= ... < 1000 DM\n",
    "- A64 : .. >= 1000 DM\n",
    "- A65 : unknown/ no savings account\n",
    "\n",
    "\n",
    "Attribute 7: (qualitative)\\\n",
    "**Present employment since** (*Emploi actuel et depuis combien de temps*)\n",
    "- A71 : unemployed\n",
    "- A72 : ... < 1 year\n",
    "- A73 : 1 <= ... < 4 years\n",
    "- A74 : 4 <= ... < 7 years\n",
    "- A75 : .. >= 7 years\n",
    "\n",
    "Attribute 8: (numerical)\\\n",
    "**Installment rate** in percentage of disposable income (*Taux de versement en pourcentage du revenu disponible*)\n",
    "\n",
    "\n",
    "Attribute 9: (qualitative)\\\n",
    "**Personal status and sex** (*Statut marital et sexe de l'individu*)\n",
    "- A91 : male : divorced/separated\n",
    "- A92 : female : divorced/separated/married\n",
    "- A93 : male : single\n",
    "- A94 : male : married/widowed\n",
    "- A95 : female : single\n",
    "\n",
    "\n",
    "Attribute 10: (qualitative)\\\n",
    "**Other debtors/guarantors** (*Autres débiteurs/garants*)\n",
    "- A101 : none\n",
    "- A102 : co-applicant\n",
    "- A103 : guarantor\n",
    "\n",
    "\n",
    "Attribute 11: (numerical)\\\n",
    "**Present residence since** (*Résidence actuelle depuis combien de temps*)\n",
    "\n",
    "\n",
    "Attribute 12: (qualitative)\\\n",
    "**Property** (*Propriété*)\n",
    "    - A121 : real estate\n",
    "    - A122 : if not A121 : building society savings agreement/ life insurance\n",
    "    - A123 : if not A121/A122 : car or other, not in attribute 6\n",
    "    - A124 : unknown / no property\n",
    "\n",
    "\n",
    "Attribute 13: (numerical)\\\n",
    "**Age** in years (*Âge en années*)\n",
    "\n",
    "\n",
    "Attribute 14: (qualitative)\\\n",
    "**Other installment plans** (*Autres plans de paiement*)\n",
    "- A141 : bank\n",
    "- A142 : stores\n",
    "- A143 : none\n",
    "\n",
    "Attribute 15: (qualitative)\\\n",
    "**Housing** (*Logement*)\n",
    "- A151 : rent\n",
    "- A152 : own\n",
    "- A153 : for free\n",
    "\n",
    "\n",
    "Attribute 16: (numerical)\\\n",
    "**Number of existing credits** at this bank (*Nombre de crédits existants dans cette banque*)\n",
    "\n",
    "\n",
    "Attribute 17: (qualitative)\\\n",
    "**Job** (*Emploi*)\n",
    "- A171 : unemployed/ unskilled - non-resident\n",
    "- A172 : unskilled - resident\n",
    "- A173 : skilled employee / official\n",
    "- A174 : management/ self-employed / highly qualified employee / officer\n",
    "\n",
    "Attribute 18: (numerical)\\\n",
    "**Number of people being liable** to provide maintenance for (*Nombre de personnes tenus de subvenir aux besoins de l'individu en question*)\n",
    "\n",
    "Attribute 19: (qualitative)\\\n",
    "**Telephone** (*Téléphone*)\n",
    "- A191 : none\n",
    "- A192 : yes, registered under the customers name\n",
    "\n",
    "Attribute 20: (qualitative)\\\n",
    "**Foreign worker** (*Travailleur étranger*) \n",
    "- A201 : yes\n",
    "- A202 : no\n",
    "\n",
    "La variable cible `Risk` (21ème et dernière colonne) contient 2 modalités: 1 pour 'Good Risk' et 2 pour 'Bad Risk'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Remplacement des noms de colonnes actuels par des noms plus explicites\n",
    "df.columns = ['Status of existing checking account',\n",
    "              'Duration',\n",
    "              'Credit history',\n",
    "              'Purpose',\n",
    "              'Credit amount',\n",
    "              'Savings account/bonds',\n",
    "              'Present employment since',\n",
    "              'Installment rate',\n",
    "              'Personal status and sex', 'Other debtors/guarantors',\n",
    "              'Present residence since', 'Property',\n",
    "              'Age',\n",
    "              'Other installment plans',\n",
    "              'Housing', 'Number of existing credits',\n",
    "              'Job',\n",
    "              'Number of people being liable',\n",
    "              'Telephone',\n",
    "              'Foreign worker',\n",
    "              'Risk']\n",
    "\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aperçu d'un DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Affichage des 10 premières lignes du DataFrame df\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Affichage des 20 dernières lignes du DataFrame df\n",
    "df.tail(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Informations sur un DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Affichage des dimensions du DataFrame df\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Affichage de quelques informations sur df\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sélection de colonnes d'un DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Sélection de la colonne nommée 'Number of existing credits'\n",
    "df['Number of existing credits']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lorsque l'on sélectionne une seule colonne, l'objet renvoyé par Pandas est de type `Series`. Ce type particulier fonctionne de la même façon qu'une liste voire d'un array Numpy à une seule dimension.\n",
    "\n",
    "Ainsi, on peut accéder à la valeur stockée dans la ligne d'indice 4 de la Series renvoyée par la sélection de la colonne 'Number of existing credits' en procédant de cette manière :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Indexation de la Series renvoyée par la sélection de la colonne 'Number of existing credits'\n",
    "df['Number of existing credits'][4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Extraction d'un sous-DataFrame contenant les colonnes 'Credit amount' et 'Purpose'\n",
    "df[['Credit amount', 'Purpose']]\n",
    "\n",
    "#A40 : car (new)\n",
    "#A41 : car (used)\n",
    "#A42 : furniture/equipment\n",
    "#A43 : radio/television\n",
    "#A44 : domestic appliances\n",
    "#A45 : repairs\n",
    "#A46 : education\n",
    "#A47 : (vacation - does not exist?)\n",
    "#A48 : retraining\n",
    "#A49 : business\n",
    "#A410 : others"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sélection de lignes d'un DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous pouvons récupérer une ou plusieurs lignes d'un DataFrame à l'aide de la méthode `DataFrame.loc[]` en renseignant entre crochets le nom de la ou des lignes concernées, de la même manière que pour l'indexation d'une variable de type **liste**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Sélection de la ligne d'indice 855\n",
    "df.loc[855]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Afin de récupérer plusieurs lignes, nous pouvons soit :\n",
    "- renseigner une liste d'indices\n",
    "- utiliser le slicing si les indices sont uniques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sélection des lignes d'indice 0, 499 et 999\n",
    "df.loc[[0, 499, 999]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Sélection des colonnes 'Age' et 'Personal status and sex' pour les lignes d'indices 0, 499 et 999\n",
    "df.loc[[0, 499, 999], ['Age', 'Personal status and sex']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La méthode `DataFrame.iloc[]` permet d'indexer un DataFrame de la même façon qu'un array Numpy, c'est-à-dire en ne renseignant que les indexes numériques des lignes et colonnes, permettant ainsi d'utiliser le slicing sans contraintes :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Extraction des lignes d'indice 989 jusqu'à la dernière et des colonnes de la première jusqu'à la 6ème\n",
    "df.iloc[989:, :6]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dans le cas où l'indexation des lignes est celle par défaut (numérotation de `0` à `df.shape[0] - 1`), les deux méthodes précédentes sont équivalentes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Indexation Conditionnelle d'un DataFrame\n",
    "Nous pouvons, comme pour les arrays Numpy, utiliser **l'indexation conditionnelle** afin d'extraire les lignes d'un DataFrame qui vérifient une condition donnée.\n",
    "\n",
    "![Image DataScientest indexation conditionnelle Pandas](img/image_indexation_conditionnelle_pandas.png)\n",
    "\n",
    "Source : *Pandas pour la Data Science - Introduction aux DataFrames*, de [DataScientest](https://datascientest.com)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Sélection des lignes de df pour lesquels les valeurs présentes dans la colonne 'Risk' sont égales à 1\n",
    "df.loc[df['Risk'] == 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Méthodes statistiques d'un DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compte des modalités de la variable 'Purpose'\n",
    "df['Purpose'].value_counts(dropna = False)\n",
    "#A40 : car (new)\n",
    "#A41 : car (used)\n",
    "#A42 : furniture/equipment\n",
    "#A43 : radio/television\n",
    "#A44 : domestic appliances\n",
    "#A45 : repairs\n",
    "#A46 : education\n",
    "#A47 : (vacation - does not exist?)\n",
    "#A48 : retraining\n",
    "#A49 : business\n",
    "#A410 : others"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcul de la moyenne des crédits empruntés par les personnes ayant les emplois les plus qualifiés (A174)\n",
    "df.loc[df['Job'] == 'A174']['Credit amount'].mean()\n",
    "#A171 : unemployed/ unskilled - non-resident\n",
    "#A172 : unskilled - resident\n",
    "#A173 : skilled employee / official\n",
    "#A174 : management / self-employed / highly qualified employee / officer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nettoyage de données"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gestion des doublons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Récupération des lignes contenant un doublon\n",
    "df.duplicated()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cette méthode nous renvoie également un objet de la classe `Series`, comme lorsque l'on sélectionne une colonne\n",
    "\n",
    "En considérant le booléen **True** comme étant égal à **1** et **False** comme étant égale à **0**, on peut appliquer la méthode .sum() afin de compter le nombre de doublons présent dans notre DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comptage du nombre de doublons\n",
    "df.duplicated().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La méthode d'un DataFrame permettant de supprimer les doublons est `.drop_duplicates()`. Son en-tête est la suivante :\n",
    "\n",
    "```python\n",
    "drop_duplicates(subset, keep, inplace)\n",
    "```\n",
    "\n",
    "Le paramètre `subset` indique la ou les colonnes à considérer pour identifier et supprimer les doublons. Par défaut, subset = None: on considère toutes les colonnes du DataFrame.\n",
    "\n",
    "Le paramètre `keep` indique quelle entrée doit être gardée :\n",
    "- '**first**': On garde la première occurence.\n",
    "- '**last**' : On garde la dernière occurence\n",
    "- '**False**': On ne garde aucune des occurences.\n",
    "Par défaut, **keep = 'first'**.\n",
    "\n",
    "Le paramètre `inplace` (très courant dans les méthodes de la classe DataFrame), précise si l'on modifie directement le DataFrame (dans ce cas inplace = True) ou si la méthode renvoie une copie du DataFrame (inplace = False). Une méthode appliquée avec l'argument `inplace = True` est irréversible. Par défaut, `inplace = False`.\n",
    "\n",
    "Prenons pour exemple le DataFrame suivant :\n",
    "<img src=\"img/image_drop_duplicates.png\" alt=\"Image DataScientest drop_duplicates()\" width=\"400\" height=\"200\"/>\n",
    "\n",
    "Voici les entrées qui seront supprimées en fonction de la valeur du paramètre `keep` :\n",
    "\n",
    "```python\n",
    "# Suppression en ne gardant que la première occurence du doublon\n",
    "df = df.drop_duplicates(keep = 'first')\n",
    "```\n",
    "<img src=\"img/image_drop_duplicates_first.png\" alt=\"Image DataScientest drop_duplicates(keep = 'first')\" width=\"400\" height=\"200\"/>\n",
    "\n",
    "```python\n",
    "# Suppression en ne gardant que la dernière occurence du doublon\n",
    "df = df.drop_duplicates(keep = 'last')\n",
    "```\n",
    "<img src=\"img/image_drop_duplicates_last.png\" alt=\"Image DataScientest drop_duplicates(keep = 'last')\" width=\"400\" height=\"200\"/>\n",
    "\n",
    "```python\n",
    "# Suppression en ne gardant que la première occurence du doublon\n",
    "df = df.drop_duplicates(keep = 'False')\n",
    "```\n",
    "<img src=\"img/image_drop_duplicates_false.png\" alt=\"Image DataScientest drop_duplicates(keep = 'false')\" width=\"400\" height=\"200\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop_duplicates(keep = 'last')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gestion des valeurs manquantes\n",
    "\n",
    "Une **valeur manquante** est soit :\n",
    "- une valeur non renseignée.\n",
    "- une valeur qui n'existe pas. En général, elles sont issues de calculs mathématiques n'ayant pas de solution (une division par zéro par exemple).\n",
    "\n",
    "Une valeur manquante apparaît sous la dénomination **NaN** (\"**N**ot **a** **N**umber\") dans un DataFrame.\n",
    "\n",
    "Dans cette partie, nous allons voir plusieurs méthodes pour :\n",
    "- la détection des valeurs manquantes (méthodes `.isna()` et `.any()`)\n",
    "- le remplacement de ces valeurs (méthode `.fillna()`)\n",
    "- la suppression des valeurs manquantes (méthode `.dropna()`)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Détection des valeurs manquantes\n",
    "\n",
    "![Image DataScientest isna() Pandas](img/image_isna_pandas.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On détecte les COLONNES contenant au moins une valeur manquante\n",
    "df.isna().any(axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On détecte les LIGNES contenant au moins une valeur manquante\n",
    "df.isna().any(axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On utilise l'indexation conditionelle pour afficher les entrées contenant des valeurs manquantes\n",
    "df.loc[df.isna().any(axis = 1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On compte le nombre de valeurs manquantes pour chaque COLONNE\n",
    "df.isnull().sum(axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On compte le nombre de valeurs manquantes pour chaque LIGNE\n",
    "df.isnull().sum(axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remplacement des valeurs manquantes\n",
    "Il est courant de remplacer les valeurs manquantes d'une colonne de **type numérique** avec des **statistiques** comme :\n",
    "- la moyenne: mean()\n",
    "- la médiane: median()\n",
    "- le minimum/maximum: min()/max()\n",
    "\n",
    "Pour les colonnes de **type catégorielle**, on remplacera généralement les valeurs manquantes par :\n",
    "- le mode: mode() (la modalité la plus fréquente)\n",
    "- une constante ou catégorie arbitraire tel que `0` ou `-1`\n",
    " \n",
    "On peut d'ailleurs **commencer** par **sélectionner les bonnes colonnes** afin d'éviter de faire des erreurs de remplacement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Détermination de la modalité la plus fréquente (le mode) de la colonne 'Purpose'\n",
    "mode_purpose = df['Purpose'].mode()[0]\n",
    "\n",
    "print(mode_purpose)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Remplacement des valeurs manquantes de la colonne 'Purpose' par son mode\n",
    "df['Purpose'] = df['Purpose'].fillna(mode_purpose)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Calcul de l'arrondi de la moyenne d'âge des individus que nous étudions\n",
    "mean_age = round(df['Age'].dropna().mean())\n",
    "\n",
    "print(mean_age)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remplacement des valeurs manquantes de la colonne 'Age' par l'arrondi à l'entier près de sa moyenne\n",
    "df['Age'] = df['Age'].fillna(mean_age)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remplacement des valeurs manquantes de la colonne 'Telephone' par la modalité A191\n",
    "df['Telephone'] = df['Telephone'].fillna('A191')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Suppression des valeurs manquantes\n",
    "\n",
    "La méthode `.dropna(axis, how, subset, ...)` permet de supprimer les lignes ou colonnes contenant des valeurs manquantes.\n",
    "\n",
    "Le paramètre `axis` précise si on doit supprimer des lignes ou des colonnes (**0** pour les **lignes**, **1** pour les **colonnes**).\n",
    "\n",
    "Le paramètre `how` permet de préciser comment les lignes (ou les colonnes) sont supprimées :\n",
    "- **how = 'any'**: On supprime la ligne (ou colonne) si elle contient au moins une valeur manquante.\n",
    "- **how = 'all'** : On supprime la ligne (ou colonne) si elle ne contient que des valeurs manquantes.\n",
    "\n",
    "Le paramètre `subset` permet de préciser les colonnes/lignes sur lesquelles on effectue la recherche de valeurs manquantes.\n",
    "\n",
    "Exemples :\n",
    "```python\n",
    "# On supprime toutes les lignes contenant au moins une valeur manquante\n",
    "df = df.dropna(axis = 0, how = 'any')\n",
    "\n",
    "# On supprime les colonnes vides \n",
    "df = df.dropna(axis = 1, how = 'all') \n",
    "\n",
    "# On supprime les lignes ayant des valeurs manquantes dans les 3 colonnes 'col2','col3' et 'col4'\n",
    " df.dropna(axis = 0, how = 'all', subset = ['col2','col3','col4'])\n",
    " ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Suppression des entrées contenant au moins une valeur manquante pour les caractéristiques sélectionnée\n",
    "df = df.dropna(axis = 0, how = 'any', subset = ['Status of existing checking account',\n",
    "                                                'Duration',\n",
    "                                                'Credit amount',\n",
    "                                                'Savings account/bonds',\n",
    "                                                'Present employment since',\n",
    "                                                'Housing',\n",
    "                                                'Job',\n",
    "                                                'Foreign worker'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On vérifie que les colonnes de df ne contiennent plus de valeurs manquantes\n",
    "df.isna().sum(axis = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modification des éléments d'un DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remplacement des valeurs [1, 2] de la colonne 'Risk' de df par les valeurs [0, 1]\n",
    "df['Risk'] = df['Risk'].replace(to_replace = [1, 2], value = [0, 1])\n",
    "\n",
    "df['Risk'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il est parfois nécessaire de changer le **type** d'une colonne.\n",
    "\n",
    "Par exemple, il se peut que lors de l'importation d'une base de données une variable soit de type chaîne de caractères alors qu'elle est réellement de type numérique. Il suffit qu'une des entrées de la colonne soit mal reconnue et pandas considèrera que cette colonne est de type chaîne de caractères.\n",
    "\n",
    "Cela est possible grâce à la méthode `.astype()`.\n",
    "\n",
    "Les types que nous verrons le plus souvent sont:\n",
    "- str : Chaîne de caractères ('Bonjour').\n",
    "- float : Nombre à virgule flottante (1.0, 1.14123).\n",
    "- int : Nombre entier (1, 1231)\n",
    "\n",
    "Comme pour la méthode `.rename()`, `.astype()` peut prendre en argument un dictionnaire dont les clés sont les noms des colonnes concernées et les valeurs sont les nouveaux types à assigner. Cela est pratique si l'on veut modifier le type de plusieurs colonnes en même temps.\n",
    "\n",
    "Le plus souvent, on voudra directement séléctionner la colonne dont on veut modifier le type et l'écraser en lui appliquant la méthode astype.\n",
    "```python\n",
    "# Méthode 1 : Création d'un dictionnaire puis appel à la méthode astype du DataFrame\n",
    "dictionnaire = {'col_1': 'int',\n",
    "                'col_2': 'float'}\n",
    "df = df.astype(dictionnaire)\n",
    "\n",
    "# Méthode 2 : Séléction de la colonne puis appel à la méthode astype d'une Series\n",
    "df['col_1'] = df['col_1'].astype('int')\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cast du type de la colonne 'Age' (qui était de type 'float') en type entier (integer)\n",
    "df['Age'] = df['Age'].astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Traitement de données"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filtrage d'un DataFrame\n",
    "\n",
    "Filtrer consiste à **sélectionner** un sous-ensemble de lignes d'un DataFrame qui vérifient une **condition**. Le filtrage correspond à ce qu'on appelait jusqu'à maintenant l'indexation conditionelle, mais le terme \"filtrage\" est celui qui est le plus utilisé dans la gestion de bases de données.\n",
    "\n",
    "Nous ne pouvons pas utiliser les opérateurs logiques and et or pour filtrer sur plusieurs conditions. En effet, ces opérateurs créent de l'ambiguité que pandas n'est pas capable de gérer pour filtrer les lignes.\n",
    "\n",
    "Les opérateurs adaptés au filtrage sur plusieurs conditions sont les opérateurs **binaires**:\n",
    "- l'opérateur 'et' : **&**\n",
    "- l'opérateur 'ou' : **|**\n",
    "- l'opérateur 'non' : **-**\n",
    "\n",
    "Ces opérateurs sont semblables aux opérateurs logiques mais leurs méthodes d'évaluation ne sont pas les mêmes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtrage sur les individus :\n",
    "# à mauvais risque de crédit\n",
    "# ET ayant emprunté 5000 DM ou moins\n",
    "# ET dont les jobs sont parmi les plus qualifiés\n",
    "df.loc[(df['Risk'] == 1) & (df['Job'] == 'A174') & (df['Credit amount'] <= 5000)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtrage sur les indivus :\n",
    "# ayant 200 DM ou plus sur leurs comptes chèques\n",
    "# OU ayant 1000 DM ou plus sur leurs comptes carnets\n",
    "df.loc[(df['Status of existing checking account'] == 'A13') | (df['Savings account/bonds'] == 'A64')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtrage sur les indivus :\n",
    "# qui ne sont PAS sans emploi\n",
    "df.loc[-(df['Present employment since'] == 'A71')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Union de DataFrames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Concaténation de DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Stockage des informations concernant la variable cible dans un vecteur y\n",
    "y = df['Risk']\n",
    "\n",
    "# Stockage de toutes les variables explicatives du DataFrame df\n",
    "df = df.drop(['Risk'], axis = 1)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La fonction `pandas.concat()` permet de concaténer plusieurs DataFrames, c'est-à-dire les juxtaposer horizontalement ou verticalement.\n",
    "![Image DataScientest pd.concat() Pandas 1](img/image_concat_pandas_1.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Concaténation du DataFrame df et du vecteur y pour reconstituer le jeu de données initial\n",
    "df = pd.concat([df, y], axis = 1) # axis = 1 afin de concaténer horizontalement les 2 jeux de données\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Image DataScientest pd.concat() Pandas 2](img/image_concat_pandas_2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fusion de DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Création d'une nouvelle colonne 'idClient' prenant les valeurs de 0 à 1003\n",
    "df['idClient'] = range(1, df.shape[0] + 1)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Stockage des données personnelles des individus dans un DataFrame nommé df_personalInfo\n",
    "df_personalInfo = df[['Status of existing checking account',\n",
    "                      'Savings account/bonds',\n",
    "                      'Present employment since',\n",
    "                      'Personal status and sex',\n",
    "                      'Other debtors/guarantors',\n",
    "                      'Present residence since',\n",
    "                      'Property',\n",
    "                      'Age',\n",
    "                      'Housing',\n",
    "                      'Job',\n",
    "                      'Telephone',\n",
    "                      'Foreign worker',\n",
    "                      'idClient']]\n",
    "\n",
    "df_personalInfo.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Stockage des données directement liées aux crédits contractées par les individus + idClient dans df_creditInfo \n",
    "df_creditInfo = df.drop(['Status of existing checking account',\n",
    "                         'Savings account/bonds',\n",
    "                         'Personal status and sex',\n",
    "                         'Other debtors/guarantors',\n",
    "                         'Present residence since',\n",
    "                         'Property',\n",
    "                         'Age',\n",
    "                         'Housing',\n",
    "                         'Job',\n",
    "                         'Telephone',\n",
    "                         'Foreign worker'], axis = 1)\n",
    "\n",
    "df_creditInfo.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deux DataFrames peuvent être fusionnés s'ils ont une colonne en commun. Ceci se fait grâce à la méthode `DataFrame.merge(right, on, how, ...)`\n",
    "- **right**: DataFrame à fusionner avec celui qui appelle la méthode\n",
    "- **on**: Nom de la ou des colonnes des DataFrames qui serviront de référence pour la fusion, elles doivent être évidemment communes aux deux DataFrames\n",
    "- **how**: type de jointure (basées sur les jointures de la syntaxe SQL)\n",
    "\n",
    "Voici 4 exemples selon les valeurs de la jointure appliquée :\n",
    "\n",
    "- '**inner**': La jointure interne retourne les lignes dont les valeurs dans les colonnes communes sont présentes dans les deux DataFrames. Ce type de jointure est souvent déconseillé car il peut amener à la perte de beaucoup d'entrées. Parcontre, la jointure interne ne produit aucun NaN.\n",
    "<img src=\"img/image_merge_inner.png\" alt=\"Image DataScientest DataFrame.merge(how = 'inner')\" width=\"800\" height=\"400\"/>\n",
    "\n",
    "\n",
    "- '**outer**': La jointure externe fusionne la totalité des deux DataFrames. Aucune ligne ne sera supprimée. Cette méthode peut générer énormément de NaNs.\n",
    "<img src=\"img/image_merge_outer.png\" alt=\"Image DataScientest DataFrame.merge(how = 'outer')\" width=\"800\" height=\"400\"/>\n",
    "\n",
    "\n",
    "- '**left**': La jointure à gauche retourne toutes les lignes du DataFrame de gauche, et les complète avec les lignes du second DataFrame qui coïncident selon les valeurs de la colonne commune. C'est la valeur par défaut du paramètre how.\n",
    "<img src=\"img/image_merge_left.png\" alt=\"Image DataScientest DataFrame.merge(how = 'left')\" width=\"800\" height=\"400\"/>\n",
    "\n",
    "\n",
    "- '**right**' La jointure à droite retourne toutes les lignes du DataFrame de droite, et les complète avec les lignes du DataFrame de gauche qui coïncident selon les indices de la colonne commune.\n",
    "<img src=\"img/image_merge_left.png\" alt=\"Image DataScientest DataFrame.merge(how = 'left')\" width=\"800\" height=\"400\"/>\n",
    "\n",
    "N.B : Faire une jointure à gauche, une jointure à droite ou une jointure externe suivie d'un `DataFrame.dropna(how = 'any')` est équivalent à une jointure interne."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Jointure interne entre les 2 DataFrames précédents afin de reconstituer le jeu de données original\n",
    "df_merged = df_creditInfo.merge(right = df_personalInfo, on = 'idClient', how = 'inner')\n",
    "\n",
    "df_merged.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Vérification des colonnes présentes dans le DataFrame df_merge issu de la fusion des 2 jeux de données précédents.\n",
    "df_merged.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On supprime une des deux colonnes Present employment since x ou y au choix\n",
    "df_merged = df_merged.drop(['Present employment since_x'], axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Renommage de colonne\n",
    "En plus de modifier les éléments d'un DataFrame, il est possible de **renommer** ses colonnes.\n",
    "\n",
    "Cela est possible grâce à la méthode `.rename()` qui prend en argument un **dictionnaire** dont les **clés** sont les **anciens noms** et les **valeurs** sont les **nouveaux noms**. Il faut aussi renseigner l'argument **axis = 1** pour préciser que les noms à renommer sont ceux des colonnes.\n",
    "\n",
    "```python\n",
    "# Création du dictionnaire associant les anciens noms aux nouveaux noms de colonnes\n",
    "dictionnaire = {'ancien_nom1': 'nouveau_nom1',\n",
    "                'ancien_nom2': 'nouveau_nom2'}\n",
    "\n",
    "# On renomme les variables grâce à la méthode rename\n",
    "df = df.rename(dictionnaire, axis = 1)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Création du dictionnaire associant l'ancien nom au nouveau nom de la colonne que l'on veut renommer\n",
    "dict_rename = {'Present employment since_y': 'Present employment since'}\n",
    "\n",
    "# On renomme la variable concernée grâce à la méthode rename\n",
    "df_merged = df_merged.rename(dict_rename, axis = 1)\n",
    "\n",
    "df_merged.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Re-définition de l'indexage d'un DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nouvel index à utiliser\n",
    "df = df.set_index(df['idClient']).drop(['idClient'], axis = 1)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# On revient à l'indexation numérique par défaut\n",
    "df = df.reset_index()\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Suppression d'une ou plusieurs colonnes d'un DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suppression de la colonne 'idClient' nouvellement créée\n",
    "df = df.drop(['idClient'], axis = 1)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trier et ordonner les valeurs d'un DataFrame\n",
    "La méthode `sort_values(by, ascending, ...)` permet de trier les lignes d'un DataFrame selon les valeurs d'une ou de plusieurs colonnes.\n",
    "\n",
    "- le paramètre `by` permet de préciser sur quelle(s) colonne(s) le tri est effectué.\n",
    "- le paramètre `ascending` est un booléen (**True** ou **False**) détermine l'ordre croissant/décroissant du tri. Par défaut ce paramètre vaut **True**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Tri du DataFrame df sur la colonne 'Age'\n",
    "df_sorted = df.sort_values(by = 'Age', ascending = False)\n",
    "\n",
    "df_sorted.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La méthode `.sort_index()` permet de trier un DataFrame selon son index. Dans le cas où l'index est celui par défaut (numérotation) comme ici, cette méthode n'est pas très intéressante.\n",
    "\n",
    "Elle est donc souvent combinée avec la méthode `.set_index()` de pandas que l'on a vu précédemment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Utilisation de la colonne 'Purpose' pour indexer le DataFrame et suppression de la colonne doublon\n",
    "df = df.set_index(df['Purpose']).drop(['Purpose'], axis = 1)\n",
    "\n",
    "# Tri des indices du DataFrame\n",
    "df = df.sort_index()\n",
    "\n",
    "df.head(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On revient à l'indexation numérique par défaut\n",
    "df = df.reset_index()\n",
    "\n",
    "df.head(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Opération sur les valeurs d'un DataFrame\n",
    "\n",
    "Il est souvent intéressant de modifier ou aggréger les informations des colonnes d'un DataFrame à l'aide d'une opération ou d'une fonction.\n",
    "\n",
    "Ces opérations peuvent être tout type de fonction qui prend en argument une colonne. Ainsi, le module numpy est parfaitement adapté pour effectuer des opérations sur ce type d'objet.\n",
    "\n",
    "La méthode permettant d'effectuer une opération sur une colonne est la méthode apply d'un DataFrame dont l'en-tête est:\n",
    "``` python\n",
    "apply(func, axis, ...)\n",
    "\n",
    "```\n",
    "où :\n",
    "- `func` est la **fonction** à appliquer.\n",
    "- `axis` est la **dimension** (ligne ou colonne) sur laquelle l'opération doit s'appliquer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Création d'une colonne 'Accounts' contenant la concaténation des valeurs de \n",
    "# 'Status of existing checking account' et 'Savings account/bonds'\n",
    "df['Accounts'] = df.apply(lambda row: row['Status of existing checking account'] + \"-\" +  row['Savings account/bonds'], axis = 1)\n",
    "\n",
    "df['Accounts']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyse des variables numériques du jeu de données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Résumé des statistiques descriptives de base des variables quantitatives de notre jeu de données\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sélection manuelle des variables quantitatives et stockage\n",
    "df_num = df[['Duration',\n",
    "             'Credit amount',\n",
    "             'Installment rate',\n",
    "             'Present residence since',\n",
    "             'Age',\n",
    "             'Number of existing credits',\n",
    "             'Number of people being liable']]\n",
    "\n",
    "df_num.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Sélection des variables quantitatives à l'aide de la méthode select_dtypes\n",
    "df_num = df.select_dtypes(include = ['int', 'float', 'int32', 'float32', 'int64', 'float64'])\n",
    "\n",
    "df_num.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suppression de la variable 'Risk' dans df_num car elle n'est pas quantitative (numerical)\n",
    "df_num = df_num.drop(['Risk'], axis = 1)\n",
    "\n",
    "df_num.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Indicateurs de position :\n",
    "- moyenne\n",
    "- médiane\n",
    "- quantiles\n",
    "- min\n",
    "- max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats_num = pd.DataFrame(data = df_num.mean(), columns = ['moyenne'])\n",
    "stats_num = stats_num.round(2)\n",
    "\n",
    "stats_num\n",
    "# 1 DM (Deutsche Mark) correspondait environ à 3.35 francs français en 1999 lorsque l'euro a été adopté en Allemagne\n",
    "# 1 euro correspondait environ à 2 DM en 1999 lorsque l'euro a été adopté en Allemagne"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On peut remarquer notamment que le **taux de versement moyen** est d'environ **3% des revenus disponibles**, ce qui peut laisser prétendre que les individus présents dans ce jeu de données seront en majorité capables de rembourser leurs crédits.\n",
    "\n",
    "On voit de plus que la **durée moyenne** pour rembourser est d'environ **21 mois** pour un **montant moyen** d'environ **3270 DM**.\n",
    "\n",
    "On peut alors approximer, en faisant évidemment abstraction du taux d'intérêt de ces crédits, la moyenne des salaires des individus :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "moyenne_credit_amount = stats_num.loc['Credit amount']['moyenne']\n",
    "moyenne_duration = stats_num.loc['Duration']['moyenne']\n",
    "moyenne_installment_rate = stats_num.loc['Installment rate']['moyenne']\n",
    "\n",
    "moyenne_salary = moyenne_credit_amount / moyenne_duration * 100 / moyenne_installment_rate\n",
    "\n",
    "print(\"Le salaire moyen des individus de notre jeu de données est approximativement de {} DM\".format(round(moyenne_salary, 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "stats_num['médiane'] = df_num.median()\n",
    "\n",
    "stats_num['médiane']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La moitié des crédits accordés sont d'un montant inférieurs à 2319 euros, ce qui est relativement inférieur à la moyenne, on en déduit donc qu'une majorité de crédit sont très faibles mais que la moyenne est tirée vers le haut par quelques crédits dont le montant est élevé.\n",
    "\n",
    "D'un autre côté, la durée médiane est de 18 mois et l'âge moyen des clients est de 33 ans, ce qui est très proche des valeurs moyennes, cela laisse présager que le jeu de données est donc équilibré au niveau de ces caractéristiques-là."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats_num['diff_moy_med'] = abs(stats_num['moyenne'] - stats_num['médiane'])\n",
    "\n",
    "stats_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats_num[['q1', 'q2', 'q3']] = df_num.quantile(q = [0.25, 0.5, 0.75]).transpose()\n",
    "\n",
    "stats_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "stats_num['min'] = df_num.min()\n",
    "stats_num['max'] = df_num.max()\n",
    "stats_num['diff_max_min_diff'] = stats_num['max'] - stats_num['min']\n",
    "\n",
    "stats_num"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les données de la variable 'Credit amount' sont étalées sur de large plage de valeurs ce qui accrédite l'idée d'une hétérogénéité des valeurs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Indicateurs de dispersion : écart-type et variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcul de l'écart-type de la variable 'Duration'\n",
    "ecart_type_duration = df_num['Duration'].std()\n",
    "\n",
    "intervalle_moyenneEcartType = [moyenne_duration - ecart_type_duration, moyenne_duration + ecart_type_duration]\n",
    "\n",
    "print(\"L'intervalle moyenne +/- écart type de la variable 'Duration' est :\", intervalle_moyenneEcartType)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ajout d'une nouvelle colonne intitulée 'top_Duration' à df_num contenant les valeurs de 'Duration' topés 1\n",
    "# si elles appartiennent à l'intervalle calculé précédemment et 0 sinon\n",
    "df_num['top_Duration'] = df_num['Duration'].between(intervalle_moyenneEcartType[0], intervalle_moyenneEcartType[1]).astype(int)\n",
    "\n",
    "df_num['top_Duration'].value_counts(normalize = True) * 100 # affichage de la répartition en pourcentage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Affichage des lignes de df_num qui ont une valeur 0 pour 'top_Duration' et stockage dans un DataFrame df_outliers\n",
    "df_outliers = df_num[df_num['top_Duration'] == 0]\n",
    "\n",
    "df_outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Affichage de la moyenne et de la médiane de 'Duration' pour le jeu de données df_outliers\n",
    "print(\"Moyenne du jeu de données df_outliers :\", df_outliers['Duration'].mean())\n",
    "print(\"Médiane du jeu de données df_outliers :\", df_outliers['Duration'].median())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La moyenne est très élevé: presque 2 fois plus que la moyenne globale trouvée précédemment. Il en est de même pour la médiane.\n",
    "Ce procédé nous a permis d'identifier les valeurs extrêmes de 'Duration'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcul des quintiles de la variable 'Duration' \n",
    "df_num['Duration_quantiles'] = pd.qcut(df_num['Duration'], q = 4, labels = ['20%','40%','60%','80%'])\n",
    "\n",
    "df_quantiles_duration = pd.DataFrame(columns = ['20%','40%','60%','80%'], index = ['Quantiles de Duration'])\n",
    "\n",
    "# Stockage des moyennes des valeurs de chaque quantiles dans un DataFrame intitulé df_quantiles_duration\n",
    "for i in df_num['Duration_quantiles'].unique():\n",
    "    df_quantiles_duration.loc['Quantiles de Duration', i] = df_num[df_num['Duration_quantiles'] == i]['Duration'].mean()\n",
    "    \n",
    "df_quantiles_duration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On peut constater une importante différence entre les quintile à 60% et celui à 80%, ce qui signifie que le dernier cinquième des valeurs évolue sur des valeurs bien plus élevées que le reste."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Indicateurs de forme : le skewness (pour mesurer l'asymétrie d'un jeu de données) et le kurtosis (pour mesurer l'aplatissement)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Calcul du coefficient d'asymétrie (skewness)\n",
    "stats_num[\"coeff d'asymétrie\"] = df_num.skew()\n",
    "\n",
    "stats_num"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le coefficient d'asymétrie des variables 'Credit amount' et 'Number of people being liable' est élevé (environ 2), ce qui signifie que la majorité de leurs valeurs sont au-dessus de leur valeur moyenne."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcul du coefficient d'aplatissement (kurtosis)\n",
    "stats_num[\"coeff d'applatissement\"] = df_num.kurtosis()\n",
    "\n",
    "stats_num"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On remarque que la distribution des montants de crédits accordés a un coefficient d'applatissement élevé (+ de 4), ce qui signifie que sa distribution est très concentrée, avec une très grande majorité de valeurs autour de sa moyenne.\n",
    "\n",
    "A contrario, les distributions des variables 'Installment rate' et 'Present residence since' sont très aplaties, ce qui signifie que si on compte toutes les valeurs qu'elles peuvent prendre, nous allons tomber sur à peu près le même compte pour chaque valeur. Le kurtosis étant rarement négatif pour une variable quantitative, cela nous amène à penser que ces 2 variables sont en fait catégorielles et comprennent donc un nombre limités de modalités."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df['Installment rate'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df['Present residence since'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suppression des lignes correspondant aux variables\n",
    "# ['Installment rate', 'Present residence since', 'Number of existing credits', 'Number of people being liable']\n",
    "# du DataFrame stats_num\n",
    "\n",
    "stats_num = stats_num.loc[['Duration', 'Credit amount', 'Age']]\n",
    "\n",
    "stats_num"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyse des variables qualitatives du jeu de données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sélection des variables catégorielles à l'aide de la méthode select_dtypes\n",
    "df_cat = df.select_dtypes(include = ['O'])\n",
    "\n",
    "df_cat.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Sélection manuelle des variables catégorielles\n",
    "df_cat = df[['Status of existing checking account',\n",
    "             'Credit history',\n",
    "             'Purpose',\n",
    "             'Savings account/bonds',\n",
    "             'Present employment since',\n",
    "             'Personal status and sex',\n",
    "             'Other debtors/guarantors',\n",
    "             'Property',\n",
    "             'Other installment plans',\n",
    "             'Housing',\n",
    "             'Job',\n",
    "             'Telephone',\n",
    "             'Foreign worker',\n",
    "             'Accounts',\n",
    "             'Installment rate',\n",
    "             'Present residence since',\n",
    "             'Number of existing credits',\n",
    "             'Number of people being liable',\n",
    "             'Risk']]\n",
    "\n",
    "df_cat.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Affichage des fréquences des modalités de la variable 'Risk'\n",
    "df_cat['Risk'].value_counts(normalize = True) * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On constate que 70% des crédits accordés représentent un mauvais risque. Ce chiffre a peu d'intérêt si on ne le met pas en perspective avec le montant des crédits accordés et leurs durées."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Découpage des montants de crédits en 4 classes selon les 3 quantiles de la nouvelle variable 'amount_classes' créée\n",
    "df['amount_classes'] = pd.qcut(df['Credit amount'], q = 4, labels = [0, 1, 2, 3])\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grouper les éléments d'un DataFrame\n",
    "\n",
    "La méthode `groupby` permet de **grouper les lignes** d'un DataFrame qui partagent une valeur commune sur une colonne.\n",
    "\n",
    "Cette méthode ne renvoie pas un DataFrame. L'objet renvoyé par la méthode groupby est un objet de la classe **DataFrameGroupBy**.\n",
    "\n",
    "Cette classe permet de réaliser des opérations comme le calcul de statistiques (somme, moyenne, maximum, etc.) pour chaque modalité de la colonne sur laquelle on groupe les lignes.\n",
    "\n",
    "La structure générale d'une **opération groupby** est la suivante:\n",
    "- **Séparation** des données (**Split**).\n",
    "- **Application** d'une fonction (**Apply**).\n",
    "- **Combinaison** des résultats (**Combine**).\n",
    "\n",
    "image_groupby_pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "resume_credit_amount = df.groupby(['amount_classes', 'Risk']).agg({'Credit amount': 'sum'})\n",
    "\n",
    "resume_credit_amount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in [0, 1, 2, 3]:\n",
    "    somme_amount = df.loc[df['amount_classes'] == i].sum()\n",
    "    resume_credit_amount.loc[i, 0] = resume_credit_amount.loc[i, 0] / somme_amount\n",
    "    resume_credit_amount.loc[i, 1] = resume_credit_amount.loc[i, 1] / somme_amount\n",
    "    \n",
    "resume_credit_amount"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On remarque qu'il y a environ 45% des crédits dont le montant se situe dans le troisième quartile ( > 3975.5 DM) qui ne sont pas remboursés et correspondent donc à un mauvais risque alors qu'environ 78% des crédits situés dans le deuxième quartile ( < 2319 DM) sont correctement remboursés et correspondent à un bon risque.\n",
    "\n",
    "Les crédits dont les montants sont les plus faibles ( < 1364 DM) présentent un plus gros risques que ceux cités précédemment avec environ 31% de défauts de paiements.\n",
    "\n",
    "On ne peut donc pas déterminer une tendance sur la base de ces simples statistiques, il faut donc croiser cela avec d'autres variables pour véritablement comprendre quels sont les crédits qui présentent les plus gros risques pour l'entreprise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Découpage des durées de crédits en 4 classes selon les 3 quantiles de la nouvelle variable 'duration_classes' créée\n",
    "df['duration_classes'] = pd.qcut(df['Duration'], q = 4, labels = [0, 1, 2, 3])\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resume_credit_duration = df.groupby(['duration_classes', 'Risk']).agg({'Duration': 'sum'})\n",
    "\n",
    "for i in [0, 1, 2, 3]:\n",
    "    somme_duration = df[df['duration_classes'] == i].sum()\n",
    "    resume_credit_duration.loc[i, 0] = resume_credit_duration.loc[i, 0] / somme_duration\n",
    "    resume_credit_duration.loc[i, 1] = resume_credit_duration.loc[i, 1] / somme_duration\n",
    "    \n",
    "resume_credit_duration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En creusant la variable 'Duration', on remarque une tendance qui nous amène à croire que plus les crédits sont longs à rembourser, plus ils seront classifiés comme mauvais risque.\n",
    "\n",
    "Les pourcentages pour le troisième quartile sont très similaires à ceux du troisième quartile de la variable 'Credit amount' étudié précédemment, il faut donc vérifier s'il s'agit bien des mêmes crédits pour ces 2 variables-là."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fréquences des crédits dont les montants et les durées sont supérieurs à la valeur de leurs troisièmes quartiles\n",
    "df.loc[(df['Credit amount'] > 3972.5) & (df['Duration'] > 24)]['Risk'].value_counts(normalize = True) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Comptage des crédits dont les montants et les durées sont supérieurs à la valeur de leurs troisièmes quartiles\n",
    "df.loc[(df['Credit amount'] > 3972.5) & (df['Duration'] > 24)]['Risk'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(70 / 301, 83 / 702)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(70 / 301) / (70 / 301 + 83 / 702)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Peut-on avoir une idée de l'intervalle d'âge le plus risqué ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "age_cible = df.groupby(['Risk']).agg({'Age': ['median', 'mean', 'min', 'max']})\n",
    "\n",
    "age_cible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "age_cible['Age', 'diff_max_min'] = age_cible.loc[:, (['Age'], ['max'])].values - age_cible.loc[:, (['Age'], ['min'])].values\n",
    "\n",
    "age_cible"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les statistiques en rapport avec l'âge des clients sont pratiquement les mêmes qu'ils soient solvables ou non, avec une moyenne très proche de la médiane, on ne peut donc pas raisonnablement cibler une classe d'âge plus sensiblement plus risqué qu'une autre."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Est-ce qu'une personne ayant peu d'argents sur ses comptes est plus risquée qu'une autre ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#A11 : ... < 0 DM\n",
    "#A12 : 0 <= ... < 200 DM\n",
    "#A13 : ... >= 200 DM / salary assignments for at least 1 year\n",
    "#A14 : no checking account\n",
    "\n",
    "resume_checking_account_status = df.groupby(['Status of existing checking account', 'Risk']).agg({'Status of existing checking account': 'count'})\n",
    "\n",
    "for i in ['A11', 'A12', 'A13', 'A14']:\n",
    "    compte_checking_account_status = df.loc[df['Status of existing checking account'] == i].count()\n",
    "    resume_checking_account_status.loc[i, 0] = resume_checking_account_status.loc[i, 0] / compte_checking_account_status\n",
    "    resume_checking_account_status.loc[i, 1] = resume_checking_account_status.loc[i, 1] / compte_checking_account_status\n",
    "\n",
    "resume_checking_account_status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#A61 : ... < 100 DM (Deutsche Mark)\n",
    "#A62 : 100 <= ... < 500 DM\n",
    "#A63 : 500 <= ... < 1000 DM\n",
    "#A64 : .. >= 1000 DM\n",
    "#A65 : unknown/ no savings account\n",
    "\n",
    "resume_savings_account = df.groupby(['Savings account/bonds', 'Risk']).agg({'Savings account/bonds': 'count'})\n",
    "\n",
    "for i in ['A61', 'A62', 'A63', 'A64', 'A65']:\n",
    "    compte_savings_account = df.loc[df['Savings account/bonds'] == i].count()\n",
    "    resume_savings_account.loc[i, 0] = resume_savings_account.loc[i, 0] / compte_savings_account\n",
    "    resume_savings_account.loc[i, 1] = resume_savings_account.loc[i, 1] / compte_savings_account\n",
    "\n",
    "resume_savings_account"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Est-ce qu'une catégorie de crédit est plus risquée qu'une autre ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#A40 : car (new)\n",
    "#A41 : car (used)\n",
    "#A42 : furniture/equipment\n",
    "#A43 : radio/television\n",
    "#A44 : domestic appliances\n",
    "#A45 : repairs\n",
    "#A46 : education\n",
    "#A47 : (vacation - does not exist?)\n",
    "#A48 : retraining\n",
    "#A49 : business\n",
    "#A410 : others\n",
    "\n",
    "resume_credit_purpose = df.groupby(['Purpose', 'Risk']).agg({'Purpose': 'count'})\n",
    "\n",
    "for i in ['A40', 'A41', 'A42', 'A43', 'A44', 'A45', 'A46', 'A48', 'A49', 'A410']:\n",
    "    compte_credit_purpose = df.loc[df['Purpose'] == i].count()\n",
    "    resume_credit_purpose.loc[i, 0] = resume_credit_purpose.loc[i, 0] / compte_credit_purpose\n",
    "    resume_credit_purpose.loc[i, 1] = resume_credit_purpose.loc[i, 1] / compte_credit_purpose\n",
    "\n",
    "resume_credit_purpose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "comptage_credit_purpose = df.groupby(['Purpose', 'Risk']).agg({'Purpose': 'count'})\n",
    "\n",
    "comptage_credit_purpose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "stats_credit_purpose = df.groupby(['Purpose', 'Risk']).agg({'Credit amount': 'sum', 'Duration': 'sum'})\n",
    "stats_credit_purpose['Montant par mois'] = stats_credit_purpose['Credit amount'] / stats_credit_purpose['Duration']\n",
    "\n",
    "stats_credit_purpose\n",
    "#A41 : car (used)\n",
    "#A410 : others\n",
    "#A49 : business"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ce tableau nous montre qu'en montant, sur tout l'argent qui a été prêté pour toutes les catégories, il y a plus d'argents qui n'a pas été remboursé sur les crédits des catégories A41, A410 et A49 que d'argent qui a été correctement remboursé."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Credit amount'].sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyse des corrélations entre les variables du jeu de données"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L'analyse des liaisons entre variables passe par l'étude des corrélations entre elles. Il faut distinguer 3 niveaux d'analyse : entre les variables quantitatives, entre les variables qualitatives, entre les variables qualitatives et quantitatives. Pour chaque niveau d'analyse, il faut répondre à cette question : y a-t-il dépendance ou indépendance entre les variables et dans quelle mesure ?\n",
    "\n",
    "Pour tester l'indépendance de variables lorsque les variables sont quantitatives, le test de corrélation de Pearson s'impose.\n",
    "\n",
    "Un test statistique est une procédure de décision entre deux hypothèses. Il s'agit d'une démarche consistant à rejeter ou à ne pas rejeter une hypothèse statistique, appelée hypothèse nulle  $H_0$  , en fonction d'un jeu de données.\n",
    "\n",
    "N.B : \"Ne pas rejeter\" une hypothèse ne signifie pas forcément l'accepter.\n",
    "\n",
    "Dans le cas du test de corrélation de **Pearson**, l'hypothèse nulle est la suivante :\n",
    "$$\n",
    "H_0: \\text{\"Les deux variables testées sont indépendantes\"}\n",
    "$$\n",
    "\n",
    "Pour rejeter ou non cette hypothèse, on regarde la p-value du test. Si cette dernière est en dessous de 5%, on rejette $H_0$.\n",
    "\n",
    "Il convient de définir la notion de p-value : la p-value est la probabilité, sous $H_0$, d’obtenir une statistique aussi extrême (pour ne pas dire aussi grande) que la valeur observée sur l’échantillon. Elle représente la probabilité de rejeter l'hypothèse nulle si elle est vraie. Plus la p-value est petite, plus la probabilité de faire une erreur en rejetant l'hypothèse nulle est faible.\n",
    "\n",
    "Pour mesurer la corrélation entre les deux variables, on s'appuiera sur le coefficient de corrélation de **Pearson**.\n",
    "\n",
    "Le coefficient de corrélation de Pearson est une formule qui permet de quantifier la relation linéaire entre deux variables : le coefficient est un réel entre -1 et 1 avec :\n",
    "- 1 les variables sont corrélées\n",
    "- 0 les variables sont décorrélées\n",
    "- -1 les variables sont corrélées négativement\n",
    "\n",
    "Des corrélations positives impliquent qu'à mesure que x augmente, il en va de même pour y. Les corrélations négatives impliquent que lorsque x augmente, y diminue.\n",
    "\n",
    "Le coefficient de Pearson est obtenu par la formule :\n",
    "$$\n",
    "pearsonr = \\frac{cov(X, Y)}{\\sigma_X.\\sigma_Y}\n",
    "$$\n",
    "\n",
    "- $cov(X,Y)$ est la covariance entre $X$ et $Y$\n",
    "- $\\sigma_X$ l'écart type de $X$\n",
    "- $\\sigma_Y$ l'écart type de $Y$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import du sous-module stats de la librairie scipy qui permet d'effectuer des tests statistiques\n",
    "import scipy.stats as st"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test de corrélation de Pearson entre les variables 'Credit amount' et 'Duration'\n",
    "pearsonr_age_creditAmount = st.pearsonr(df['Credit amount'], df['Duration'])\n",
    "print(\"Le test de corrélation de Pearson entre les variables 'Credit amount' et 'Duration' peut se résumer de la manière suivante :\",\n",
    "      pearsonr_age_creditAmount)\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"La valeur du coefficient de Pearson de ce test est de\", pearsonr_age_creditAmount[0])\n",
    "print(\"La p-value du test de corrélation de Pearson est de\", pearsonr_age_creditAmount[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La p-value est inférieure à 5% et le coefficient est proche de 1, il y a donc une corrélation modérée entre les deux variables.\n",
    "\n",
    "On peut alors en déduire qu'en général, lorsque le montant d'un crédit augmente, sa durée aussi."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Affichage de la matrice de corrélation (de Pearson par défaut) de notre jeu de données df\n",
    "matrice_correlation = df.corr()\n",
    "\n",
    "matrice_correlation = matrice_correlation.loc[['Age', 'Credit amount', 'Duration']].drop(['Installment rate',\n",
    "                                                                                          'Present residence since',\n",
    "                                                                                          'Number of existing credits',\n",
    "                                                                                          'Number of people being liable',\n",
    "                                                                                          'Risk'], axis = 1)\n",
    "\n",
    "matrice_correlation = matrice_correlation[['Age', 'Credit amount', 'Duration']]\n",
    "\n",
    "matrice_correlation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les variables 'Age' et 'Credit amount' ainsi que 'Age' et 'Duration' sont fortement décorrélées (pearsonr ~ 0.0). Cela signifie que l'âge d'une personne n'influence pas du tout le montant d'un crédit ni sa durée."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lorsque les variables sont qualitatives, un test de corrélation de Pearson n'est alors pas adapté. Pour cela, on commence par utiliser la table de contingence. On appelle une table de contingence, la table croisée contenant les différentes catégories des deux variables en question.\n",
    "\n",
    "Pour afficher une table de contingence, il faut utiliser la fonction `pandas.crosstab` avec les deux variables en argument.\n",
    "\n",
    "Cette table de contingence permet de visualiser comment se distribuent les catégories de la variable 1 au sein de la variable 2. Pour des variables totalement indépendantes, la distribution doit être proportionnelle entre chaque ligne et entre chaque colonne."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_contingence_risk_purpose = pd.crosstab(df['Risk'], df['Purpose'])\n",
    "\n",
    "table_contingence_risk_purpose"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le test qu'on effectue alors est un test du $\\chi^2$  par table de contingence. Pour chaque case d'un tableau, il effectue un test du $\\chi^2$ (test de proportions) entre l'effectif de la case et l'effectif total de la colonne. La statistique de test est alors obtenue en faisant la somme de toutes ces statistiques.\n",
    "\n",
    "Pour ce test, on pose comme hypothèse nulle:\n",
    "$$\n",
    "H_0: \\text{\"les variables maincategory et state sont indépendantes\"}\n",
    "$$\n",
    "\n",
    "Ce test peut s'effectuer tout simplement grâce à la fonction chi2_contingency de scipy appliquée à la table de contingence. Elle renvoie un array de 4 éléments : la statistique du test, la p-value, le degré de liberté et la liste des fréquences attendues. Pour rejeter l'hypothèse nulle, il est nécessaire que la p-value soit inférieure à 5%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_chi2_risk_purpose = st.chi2_contingency(table_contingence_risk_purpose)\n",
    "\n",
    "chi_2_risk_purpose = test_chi2_risk_purpose[0]\n",
    "p_value_risk_purpose = test_chi2_risk_purpose[1]\n",
    "ddl_risk_purpose = test_chi2_risk_purpose[2]\n",
    "\n",
    "print(chi_2_risk_purpose, p_value_risk_purpose, ddl_risk_purpose)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comme pour les variables quantitatives, on peut mesurer le niveau de corrélation entre deux variables qualitatives. Pour cela, on utilise le V de Cramer corrigé pour contrer le biais qui utilise les résulats du test du $\\chi^2$ . Il renvoie une valeur entre 0 et 1. Il se calcule comme ceci :\n",
    "\n",
    "![Image DataScientest formule V de Cramer](img/image_V_Cramer.png)\n",
    "avec :\n",
    "- $\\chi^2$ la statistique du test du $\\chi^2$ .\n",
    "- N le nombre d'observations du jeu de données.\n",
    "- k le nombre de colonne du tableau de contingence.\n",
    "- r le nombre de ligne du tableau de contingence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import de la fonction sqrt du module math\n",
    "from math import sqrt\n",
    "\n",
    "# Définition d'une fonction V_cramer qui prend en argument un tableau de contingence, le nombre d'observation et renvoie la valeur du V de Cramer\n",
    "def V_cramer(table_contingence, N):\n",
    "    \"\"\"\n",
    "    Cette fonction calcule la valeur du V de Cramer.\n",
    "\n",
    "    Paramètres:\n",
    "        table_contingence : la table de contingence des variables à tester.\n",
    "        N : le nombre d'observations du DataFrame contenant les variables à tester.\n",
    "\n",
    "    Renvoie:\n",
    "        La valeur du V de Cramer.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Détermination de la statistique du test du χ², du nombre de colonne k et du nombre ligne du tableau de contingence r\n",
    "    statistique_chi2 = st.chi2_contingency(table_contingence)[0]\n",
    "    k = table_contingence.shape[1]\n",
    "    r = table_contingence.shape[0]\n",
    "    \n",
    "    # Calcul de phi²\n",
    "    phi2 = max(0, (statistique_chi2 / N - (((k - 1)*(r - 1)) / (N - 1))))\n",
    "    \n",
    "    # Calcul de k_tilde\n",
    "    k_tilde = k - (np.square(k - 1) / (N - 1))\n",
    "    \n",
    "    # Calcul de r_tilde\n",
    "    r_tilde = r - (np.square(r - 1) / (N - 1))\n",
    "    \n",
    "    # Calcul du V de Cramer\n",
    "    V_cramer_value = np.sqrt(phi2 / min(k_tilde - 1, r_tilde - 1))\n",
    "    \n",
    "    return V_cramer_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Application de la fonction V_cramer à la table de contingence précédente\n",
    "V_cramer(table_contingence_risk_purpose, df.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le V de Cramer n'est pas très élevé. On en déduit qu'il n'y a pas une forte corrélation entre les deux variables mais qu'elle n'est pas non plus négligeable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Une dernière étape consiste à étudier les relations entre variables quantitatives et qualitatives.\n",
    "\n",
    "Dans ce but, on utilisera l'analyse de la variance (**ANOVA**) à un facteur qui permet de comparer les moyennes d'échantillon.\n",
    "\n",
    "L'objectif de ce test est de conclure sur l'influence d'une variable explicative catégorielle sur la loi d'une variable continue à expliquer.\n",
    "\n",
    "Considérons une variable catégorielle telle que 'Installment rate' et la variable numérique 'Duration'. 'Installment rate' compte 4 modalités différentes. On définit les moyennes $\\mu_1, \\mu_2, \\mu_3, \\mu_4$ qui correspondent à la moyenne des durées de crédits (**Duration**) pour les 4 modalités. Le raisonnement simple que l'on fait avec ANOVA est que si la variable 'Installment rate' n'a pas d'incidence sur 'Duration', la moyenne devrait être identique pour les 4 modalités soient $\\mu_1 = \\mu_2 = \\mu_3 = \\mu_4$.\n",
    "\n",
    "On définit donc l'hypothèse nulle :\n",
    "$$\n",
    "H_0:\"\\mu_1 = \\mu_2 = \\mu_3 = \\mu_4\"\n",
    "$$\n",
    "\n",
    "Dans la pratique la méthode **ANOVA** s'intéresse à la variation interclasse, intraclasse et à la variation totale notées respectivement SCE, SCR et SCT. Considérons les modalités d'une variable qualitative au nombre de k et  nini  les effectifs de chacune des modalités i :\n",
    "![Image DataScientest SCE](img/image_SCE.png)\n",
    "\n",
    "Il est connu que cette statistique suit une loi de Fisher de paramètre $(k-1, n-k)$.  En s'appuyant sur la valeur de cette statistique et de la p-value associé, on peut conclure sur l'influence ou non de la variable 'Installment rate' sur la variable 'Duration'.\n",
    "\n",
    "On rejette $H_0$ si la p-value est inférieure à 5%. Rejeter $H_0$ signifie ici rejeter l'hypothèse selon laquelle 'Installment rate' n'influe pas sur 'Duration'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Création du dictionnaire associant l'ancien nom au nouveau nom de la colonne que l'on veut renommer\n",
    "dict_rename_installmentRate = {'Installment rate': 'Installment_rate'}\n",
    "\n",
    "# On renomme la variable concernée grâce à la méthode rename\n",
    "df = df.rename(dict_rename_installmentRate, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import du module statsmodels.api permettant d'effectuer une analyse de la variance\n",
    "import statsmodels.api\n",
    "\n",
    "# Etude de la relation entre 'Duration' et 'Installment_rate' via l'utilisation d'ANOVA\n",
    "anova_duration_installmentRate = statsmodels.formula.api.ols('Duration ~ Installment_rate', data = df).fit()\n",
    "table_anova_duration_installmentRate = statsmodels.api.stats.anova_lm(anova_duration_installmentRate)\n",
    "\n",
    "# Affichage des résultats de l'ANOVA\n",
    "table_anova_duration_installmentRate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La p-value PR(>F) est inférieure à 5% donc on rejette l'hypothèse selon laquelle la variable 'Duration' n'influe pas sur la variable 'Installment_rate'.\n",
    "\n",
    "Cela signifie donc que la durée d'un crédit influe effectivement sur son taux de versement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Création du dictionnaire associant l'ancien nom au nouveau nom de la colonne que l'on veut renommer\n",
    "dict_rename_creditAmount = {'Credit amount': 'Credit_amount'}\n",
    "\n",
    "# On renomme la variable concernée grâce à la méthode rename\n",
    "df = df.rename(dict_rename_creditAmount, axis = 1)\n",
    "\n",
    "#A171 : unemployed/ unskilled - non-resident\n",
    "#A172 : unskilled - resident\n",
    "#A173 : skilled employee / official\n",
    "#A174 : management/ self-employed / highly qualified employee / officer\n",
    "df['Job'] = df['Job'].replace(to_replace = ['A171', 'A172', 'A173', 'A174'], value = [171, 172, 173, 174])\n",
    "\n",
    "# Etude de la relation entre 'Job' et 'Credit_amount' via l'utilisation d'ANOVA\n",
    "anova_job_creditAmount = statsmodels.formula.api.ols('Job ~ Credit_amount', data = df).fit()\n",
    "table_anova_job_creditAmount = statsmodels.api.stats.anova_lm(anova_job_creditAmount)\n",
    "\n",
    "# Affichage des résultats de l'ANOVA\n",
    "table_anova_job_creditAmount"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comme précédemment, la p-value est inférieure à 5%, on peut donc dire avec certitude que le type d'emploi d'un individu influe sur le montant du crédit qu'il va emprunter."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
